##  MLOps GPU Inference Platform Using Nvidia Triton Inference Server 

### Enterprise-Grade Scalable Inference using NVIDIA Triton & AWS EKS
"If it isn‚Äôt observable, automated, and scalable, it isn‚Äôt production-ready."

This project is an end-to-end implementation of a high-concurrency inference ecosystem designed to solve the "GPU Waste" problem. By leveraging NVIDIA Triton on AWS EKS, I‚Äôve built a system that treats AI models as first-class citizens in a microservices architecture, maximizing throughput and reducing infrastructure costs.

###  System Architecture 
#### The Stack 

- **Inference Server**: NVIDIA Triton (PyTorch, ONNX, TensorRT)
- **Orchestration**: Kubernetes (EKS/GKE)
- **Infrastructure as Code**: Terraform (GPU/CPU node groups, VPC, IAM)
- **Automation**: Python CI/CD pipelines, GitHub Actions
- **Deployment Packaging**: Helm charts (Triton, monitoring, plugins)
- **Storage**: EFS CSI Driver, S3/ECR model repo
- **Networking**: Amazon VPC CNI, Calico (optional), Istio (service mesh)
- **Observability**: Prometheus, Grafana, Loki, DCGM Exporter, `nvidia-smi`
- **Security**: IAM roles, Kubernetes RBAC, Secrets Manager/KMS, Network Policies


### üíº Business Value

This platform accelerates AI/ML adoption across industries:

- üß¨ **Biotech & Medicine** ‚Üí Enables faster drug discovery, medical imaging analysis, and precision diagnostics at scale  
- üí≥ **Finance & Insurance** ‚Üí Powers fraud detection, risk modeling, and real‚Äëtime customer insights with secure GPU inference  
- üè® **Hospitality & Retail** ‚Üí Delivers personalized recommendations, demand forecasting, and customer experience optimization  
- üåæ **Agriculture & Energy** ‚Üí Supports crop yield prediction, resource optimization, and sustainable energy analytics  
- üè≠ **Manufacturing & Logistics** ‚Üí Improves predictive maintenance, quality control, and supply chain efficiency through AI pipelines

  

<img width="800" alt="Triton-Inference" src="https://github.com/user-attachments/assets/81306c91-4d10-4e72-b516-2dedd2c43ba4" />




## üß± Architecture Overview

| Layer                  | Purpose                                                                 |
|------------------------|-------------------------------------------------------------------------|
| VPC + Subnets          | Isolated, AZ-resilient network for GPU workloads                        |
| IAM + Policies         | Fine-grained access for operators, CI/CD, and IRSA                      |
| EKS Cluster            | Managed Kubernetes control plane with GPU node groups                   |
| Node Groups            | Separate GPU and general-purpose pools for cost control                 |
| ALB Controller (IRSA)  | Ingress with service account‚Äìscoped permissions                         |
| Triton Inference       | GPU-enabled pods serving models via gRPC/HTTP                           |
| EFS                    | Shared model repository mounted into Triton pods                        |
| CI/CD                  | GitHub Actions + Terraform + Helm for secure automation                 |
| Observability          | Prometheus, Grafana, Loki, CloudWatch integration                       |
| Service Mesh           | Istio for canary rollouts                                                                 | 


## üîê Security & Compliance

- IAM roles scoped via IRSA  
- GitHub OIDC trust for CI/CD  
- Secrets managed via Kubernetes  
- zero Trust: mTLS via Istio service mesh



## üì¶ Reproducibility & Lifecycle : 

- Infrastructure-as-code via Terraform  
- Helm charts for Triton + observability stack  
- Modular teardown and rebuild workflows  
- Versioned container images via Amazon ECR
- Models hosted in the aws EFS using the kubernetes persistant volume 



## üöÄ Key Engineering Wins

1. Optimized GPU Utilization
Implemented Dynamic Batching and Instance Groups. This allows the server to group individual inference requests together on the fly, significantly increasing throughput without manual tuning.

2. Multi-Model Concurrency
Configured the ecosystem to serve multiple models (Image Classification & NLP) simultaneously on a single GPU/Node, reducing infrastructure overhead and costs‚Äîa critical requirement for Fortune 500 efficiency.

3. SRE-First Observability
Integrated custom Prometheus exporters to track:

   - Inference Latency (p95/p99)

   - Request Queue Time

   - GPU Memory Utilization

---

<img width="600" alt="Grafana-Ytiton-dashboard" src="https://github.com/user-attachments/assets/c8639bd1-4bbb-466e-a1c5-92cf450d6e6c" />



<img width="600" alt="Prometheus-Triton" src="https://github.com/user-attachments/assets/51e758eb-ccc0-4ead-97f9-9387fe3a3724" />


    
<img width="707" height="394" alt="Triton-Tesla-GPU" src="https://github.com/user-attachments/assets/a2417084-add7-4f4a-949d-49ff47979ba0" />

